#!/usr/bin/env python3
"""
ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã¨ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡
"""

import json
import os
import sys
import tempfile
import threading
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

import cv2
import numpy as np
import psutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.optimization.memory_optimizer import MemoryOptimizer
from src.optimization.performance_optimizer import PerformanceOptimizer
from src.utils.config import ConfigManager
from src.utils.logger import get_logger


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""

    test_name: str
    success: bool
    processing_time: float
    throughput: float
    memory_usage_mb: float
    cpu_usage_percent: float
    error_message: str = ""
    additional_metrics: dict[str, Any] = None


class SystemBenchmark:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆæœŸåŒ–

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_manager = ConfigManager(config_path)
        self.logger = get_logger(__name__)

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        self.results: list[BenchmarkResult] = []

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.system_info = self._get_system_info()

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.temp_dir = None

    def _get_system_info(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        return {
            "cpu_count": psutil.cpu_count(),
            "cpu_count_logical": psutil.cpu_count(logical=True),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "platform": sys.platform,
            "python_version": sys.version,
            "timestamp": datetime.now().isoformat(),
        }

    def setup_benchmark_environment(self) -> bool:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            self.logger.info("Setting up benchmark environment...")

            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            self.temp_dir = tempfile.mkdtemp(prefix="benchmark_")
            self.logger.info(f"Benchmark directory: {self.temp_dir}")

            # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
            dirs = ["input", "output", "temp", "models"]
            for dir_name in dirs:
                Path(self.temp_dir, dir_name).mkdir(parents=True, exist_ok=True)

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨è¨­å®šã‚’ä½œæˆ
            self._create_benchmark_config()

            self.logger.info("Benchmark environment setup completed")
            return True

        except Exception as e:
            self.logger.error(f"Failed to setup benchmark environment: {e}")
            return False

    def _create_benchmark_config(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨è¨­å®šã‚’ä½œæˆ"""
        benchmark_config = {
            "system": {
                "max_workers": psutil.cpu_count(),
                "memory_limit": f"{int(psutil.virtual_memory().total / (1024**3))}GB",
                "gpu_enabled": False,
            },
            "directories": {
                "input": f"{self.temp_dir}/input",
                "output": f"{self.temp_dir}/output",
                "temp": f"{self.temp_dir}/temp",
                "models": f"{self.temp_dir}/models",
            },
        }

        # è¨­å®šã‚’æ›´æ–°
        self.config_manager.config.update(benchmark_config)

    def create_test_video(
        self, path: str, duration_seconds: int, fps: int = 30, resolution: tuple = (1920, 1080)
    ) -> bool:
        """ãƒ†ã‚¹ãƒˆç”¨å‹•ç”»ã‚’ä½œæˆ"""
        try:
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            out = cv2.VideoWriter(path, fourcc, fps, resolution)

            total_frames = duration_seconds * fps
            for i in range(total_frames):
                # è¤‡é›‘ãªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆ
                frame = np.random.randint(0, 255, (resolution[1], resolution[0], 3), dtype=np.uint8)

                # éº»é›€ç‰Œã®ã‚ˆã†ãªçŸ©å½¢ã‚’è¿½åŠ 
                for j in range(20):
                    x = np.random.randint(0, resolution[0] - 100)
                    y = np.random.randint(0, resolution[1] - 150)
                    color = (
                        np.random.randint(0, 255),
                        np.random.randint(0, 255),
                        np.random.randint(0, 255),
                    )
                    cv2.rectangle(frame, (x, y), (x + 80, y + 120), color, -1)

                out.write(frame)

            out.release()
            return True

        except Exception as e:
            self.logger.error(f"Failed to create test video: {e}")
            return False

    def benchmark_video_processing(self) -> BenchmarkResult:
        """å‹•ç”»å‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        test_name = "Video Processing"
        self.logger.info(f"Running benchmark: {test_name}")

        try:
            # ãƒ†ã‚¹ãƒˆå‹•ç”»ä½œæˆ
            video_path = os.path.join(self.temp_dir, "input/benchmark_video.mp4")
            if not self.create_test_video(video_path, duration_seconds=30, resolution=(1280, 720)):
                raise Exception("Failed to create test video")

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–é–‹å§‹
            initial_memory = psutil.Process().memory_info().rss / (1024**2)  # MB

            # CPUä½¿ç”¨ç‡ç›£è¦–
            cpu_percent_start = psutil.cpu_percent(interval=None)

            # å‡¦ç†é–‹å§‹
            start_time = time.time()

            # å‹•ç”»å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®VideoProcessorã®ä»£ã‚ã‚Šï¼‰
            cap = cv2.VideoCapture(video_path)
            frame_count = 0
            processed_frames = []

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                processed_frame = cv2.GaussianBlur(frame, (5, 5), 0)
                processed_frames.append(processed_frame)
                frame_count += 1

                # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
                current_memory = psutil.Process().memory_info().rss / (1024**2)
                if current_memory - initial_memory > 2000:  # 2GBåˆ¶é™
                    self.logger.warning("Memory limit reached, stopping processing")
                    break

            cap.release()

            # å‡¦ç†çµ‚äº†
            processing_time = time.time() - start_time
            final_memory = psutil.Process().memory_info().rss / (1024**2)
            cpu_percent_end = psutil.cpu_percent(interval=None)

            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
            throughput = frame_count / processing_time if processing_time > 0 else 0

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            memory_usage = final_memory - initial_memory

            # CPUä½¿ç”¨ç‡ï¼ˆå¹³å‡ï¼‰
            cpu_usage = (cpu_percent_start + cpu_percent_end) / 2

            result = BenchmarkResult(
                test_name=test_name,
                success=True,
                processing_time=processing_time,
                throughput=throughput,
                memory_usage_mb=memory_usage,
                cpu_usage_percent=cpu_usage,
                additional_metrics={
                    "frames_processed": frame_count,
                    "fps": throughput,
                    "video_duration": 30,
                    "resolution": "1280x720",
                },
            )

            self.logger.info(
                f"{test_name} completed: {throughput:.2f} FPS, {memory_usage:.1f}MB memory"
            )
            return result

        except Exception as e:
            self.logger.error(f"{test_name} failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                success=False,
                processing_time=0,
                throughput=0,
                memory_usage_mb=0,
                cpu_usage_percent=0,
                error_message=str(e),
            )

    def benchmark_memory_performance(self) -> BenchmarkResult:
        """ãƒ¡ãƒ¢ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        test_name = "Memory Performance"
        self.logger.info(f"Running benchmark: {test_name}")

        try:
            memory_optimizer = MemoryOptimizer(self.config_manager)

            # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            initial_memory = memory_optimizer.get_memory_usage()

            start_time = time.time()

            # ãƒ¡ãƒ¢ãƒªé›†ç´„çš„ãªå‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            large_arrays = []
            max_arrays = 50
            arrays_created = 0

            for i in range(max_arrays):
                # 50MBç›¸å½“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                array = np.random.random((1000, 1000, 5)).astype(np.float32)
                large_arrays.append(array)
                arrays_created += 1

                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                current_memory = memory_optimizer.get_memory_usage()
                memory_increase = current_memory - initial_memory

                if memory_increase > 3000:  # 3GBåˆ¶é™
                    self.logger.info(f"Memory limit reached at iteration {i}")
                    break

                # å°‘ã—å¾…æ©Ÿ
                time.sleep(0.01)

            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
            cleanup_start = time.time()
            memory_optimizer.cleanup_memory()
            del large_arrays
            cleanup_time = time.time() - cleanup_start

            processing_time = time.time() - start_time
            final_memory = memory_optimizer.get_memory_usage()

            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡è¨ˆç®—
            peak_memory = max(initial_memory, final_memory)
            memory_efficiency = (
                (peak_memory - initial_memory) / arrays_created if arrays_created > 0 else 0
            )

            result = BenchmarkResult(
                test_name=test_name,
                success=True,
                processing_time=processing_time,
                throughput=arrays_created / processing_time if processing_time > 0 else 0,
                memory_usage_mb=peak_memory - initial_memory,
                cpu_usage_percent=psutil.cpu_percent(),
                additional_metrics={
                    "arrays_created": arrays_created,
                    "cleanup_time": cleanup_time,
                    "memory_efficiency_mb_per_array": memory_efficiency,
                    "peak_memory_mb": peak_memory,
                },
            )

            self.logger.info(
                f"{test_name} completed: {arrays_created} arrays, {memory_efficiency:.1f}MB/array"
            )
            return result

        except Exception as e:
            self.logger.error(f"{test_name} failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                success=False,
                processing_time=0,
                throughput=0,
                memory_usage_mb=0,
                cpu_usage_percent=0,
                error_message=str(e),
            )

    def benchmark_cpu_performance(self) -> BenchmarkResult:
        """CPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        test_name = "CPU Performance"
        self.logger.info(f"Running benchmark: {test_name}")

        try:
            start_time = time.time()

            # CPUé›†ç´„çš„ãªå‡¦ç†
            def cpu_intensive_task(iterations: int) -> int:
                result = 0
                for i in range(iterations):
                    result += sum(range(1000))
                return result

            # ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰æ€§èƒ½
            single_start = time.time()
            single_result = cpu_intensive_task(10000)
            single_time = time.time() - single_start

            # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æ€§èƒ½
            multi_start = time.time()
            threads = []
            thread_count = psutil.cpu_count()

            for i in range(thread_count):
                thread = threading.Thread(target=cpu_intensive_task, args=(10000 // thread_count,))
                threads.append(thread)
                thread.start()

            for thread in threads:
                thread.join()

            multi_time = time.time() - multi_start

            processing_time = time.time() - start_time

            # ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—è¨ˆç®—
            speedup = single_time / multi_time if multi_time > 0 else 0
            efficiency = speedup / thread_count if thread_count > 0 else 0

            result = BenchmarkResult(
                test_name=test_name,
                success=True,
                processing_time=processing_time,
                throughput=1 / processing_time if processing_time > 0 else 0,
                memory_usage_mb=psutil.Process().memory_info().rss / (1024**2),
                cpu_usage_percent=psutil.cpu_percent(),
                additional_metrics={
                    "single_thread_time": single_time,
                    "multi_thread_time": multi_time,
                    "thread_count": thread_count,
                    "speedup": speedup,
                    "efficiency": efficiency,
                },
            )

            self.logger.info(
                f"{test_name} completed: {speedup:.2f}x speedup, {efficiency:.2%} efficiency"
            )
            return result

        except Exception as e:
            self.logger.error(f"{test_name} failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                success=False,
                processing_time=0,
                throughput=0,
                memory_usage_mb=0,
                cpu_usage_percent=0,
                error_message=str(e),
            )

    def benchmark_io_performance(self) -> BenchmarkResult:
        """I/O ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        test_name = "I/O Performance"
        self.logger.info(f"Running benchmark: {test_name}")

        try:
            start_time = time.time()

            # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿æ€§èƒ½
            write_start = time.time()
            test_file = os.path.join(self.temp_dir, "temp/io_test.dat")

            # 100MB ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
            data_size_mb = 100
            chunk_size = 1024 * 1024  # 1MB chunks

            with open(test_file, "wb") as f:
                for i in range(data_size_mb):
                    data = np.random.bytes(chunk_size)
                    f.write(data)

            write_time = time.time() - write_start

            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ€§èƒ½
            read_start = time.time()

            with open(test_file, "rb") as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break

            read_time = time.time() - read_start

            # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.remove(test_file)

            processing_time = time.time() - start_time

            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®— (MB/s)
            write_throughput = data_size_mb / write_time if write_time > 0 else 0
            read_throughput = data_size_mb / read_time if read_time > 0 else 0

            result = BenchmarkResult(
                test_name=test_name,
                success=True,
                processing_time=processing_time,
                throughput=(write_throughput + read_throughput) / 2,
                memory_usage_mb=psutil.Process().memory_info().rss / (1024**2),
                cpu_usage_percent=psutil.cpu_percent(),
                additional_metrics={
                    "data_size_mb": data_size_mb,
                    "write_time": write_time,
                    "read_time": read_time,
                    "write_throughput_mbps": write_throughput,
                    "read_throughput_mbps": read_throughput,
                },
            )

            self.logger.info(
                f"{test_name} completed: Write {write_throughput:.1f}MB/s, Read {read_throughput:.1f}MB/s"
            )
            return result

        except Exception as e:
            self.logger.error(f"{test_name} failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                success=False,
                processing_time=0,
                throughput=0,
                memory_usage_mb=0,
                cpu_usage_percent=0,
                error_message=str(e),
            )

    def benchmark_system_optimization(self) -> BenchmarkResult:
        """ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        test_name = "System Optimization"
        self.logger.info(f"Running benchmark: {test_name}")

        try:
            performance_optimizer = PerformanceOptimizer(self.config_manager)

            start_time = time.time()

            # æœ€é©åŒ–å‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            before_metrics = performance_optimizer.get_current_metrics()

            # ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–å®Ÿè¡Œ
            optimization_result = performance_optimizer.optimize_system()

            # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            after_metrics = performance_optimizer.get_current_metrics()

            processing_time = time.time() - start_time

            # æ”¹å–„åº¦è¨ˆç®—
            cpu_improvement = (
                before_metrics.cpu_usage - after_metrics.cpu_usage
                if hasattr(before_metrics, "cpu_usage") and hasattr(after_metrics, "cpu_usage")
                else 0
            )
            memory_improvement = (
                before_metrics.memory_usage - after_metrics.memory_usage
                if hasattr(before_metrics, "memory_usage")
                and hasattr(after_metrics, "memory_usage")
                else 0
            )

            result = BenchmarkResult(
                test_name=test_name,
                success=optimization_result.success
                if hasattr(optimization_result, "success")
                else True,
                processing_time=processing_time,
                throughput=1 / processing_time if processing_time > 0 else 0,
                memory_usage_mb=after_metrics.memory_usage
                if hasattr(after_metrics, "memory_usage")
                else 0,
                cpu_usage_percent=after_metrics.cpu_usage
                if hasattr(after_metrics, "cpu_usage")
                else 0,
                additional_metrics={
                    "cpu_improvement": cpu_improvement,
                    "memory_improvement": memory_improvement,
                    "optimization_type": getattr(
                        optimization_result, "optimization_type", "general"
                    ),
                    "recommendations_count": len(
                        performance_optimizer.get_optimization_recommendations()
                    ),
                },
            )

            self.logger.info(
                f"{test_name} completed: CPU {cpu_improvement:+.1f}%, Memory {memory_improvement:+.1f}%"
            )
            return result

        except Exception as e:
            self.logger.error(f"{test_name} failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                success=False,
                processing_time=0,
                throughput=0,
                memory_usage_mb=0,
                cpu_usage_percent=0,
                error_message=str(e),
            )

    def run_all_benchmarks(self) -> dict[str, Any]:
        """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        self.logger.info("Starting comprehensive system benchmark")
        self.logger.info("=" * 60)

        if not self.setup_benchmark_environment():
            return {"success": False, "error": "Failed to setup benchmark environment"}

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmarks = [
            self.benchmark_video_processing,
            self.benchmark_memory_performance,
            self.benchmark_cpu_performance,
            self.benchmark_io_performance,
            self.benchmark_system_optimization,
        ]

        for benchmark_func in benchmarks:
            try:
                result = benchmark_func()
                self.results.append(result)

                if result.success:
                    self.logger.info(f"âœ… {result.test_name}: {result.processing_time:.2f}s")
                else:
                    self.logger.error(f"âŒ {result.test_name}: {result.error_message}")

            except Exception as e:
                self.logger.error(f"Benchmark {benchmark_func.__name__} failed: {e}")

        # çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        summary = self._generate_benchmark_summary()

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self._save_benchmark_report(summary)

        self.logger.info("=" * 60)
        self.logger.info("Benchmark completed")
        self.logger.info(f"Report saved: {report_path}")

        return {
            "success": True,
            "results": [result.__dict__ for result in self.results],
            "summary": summary,
            "report_path": report_path,
            "system_info": self.system_info,
        }

    def _generate_benchmark_summary(self) -> dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        successful_tests = [r for r in self.results if r.success]
        failed_tests = [r for r in self.results if not r.success]

        summary = {
            "total_tests": len(self.results),
            "successful_tests": len(successful_tests),
            "failed_tests": len(failed_tests),
            "success_rate": len(successful_tests) / len(self.results) if self.results else 0,
            "total_time": sum(r.processing_time for r in self.results),
            "average_throughput": np.mean([r.throughput for r in successful_tests])
            if successful_tests
            else 0,
            "average_memory_usage": np.mean([r.memory_usage_mb for r in successful_tests])
            if successful_tests
            else 0,
            "average_cpu_usage": np.mean([r.cpu_usage_percent for r in successful_tests])
            if successful_tests
            else 0,
            "performance_score": self._calculate_performance_score(),
            "recommendations": self._generate_recommendations(),
        }

        return summary

    def _calculate_performance_score(self) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not self.results:
            return 0.0

        # å„ãƒ†ã‚¹ãƒˆã®é‡ã¿
        weights = {
            "Video Processing": 0.3,
            "Memory Performance": 0.2,
            "CPU Performance": 0.2,
            "I/O Performance": 0.2,
            "System Optimization": 0.1,
        }

        total_score = 0.0
        total_weight = 0.0

        for result in self.results:
            if result.success and result.test_name in weights:
                # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
                normalized_score = min(100, result.throughput * 10)  # ç°¡æ˜“çš„ãªæ­£è¦åŒ–
                weighted_score = normalized_score * weights[result.test_name]
                total_score += weighted_score
                total_weight += weights[result.test_name]

        return total_score / total_weight if total_weight > 0 else 0.0

    def _generate_recommendations(self) -> list[str]:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        for result in self.results:
            if not result.success:
                recommendations.append(f"Fix issues in {result.test_name}: {result.error_message}")
            elif result.throughput < 1.0:
                recommendations.append(
                    f"Consider optimizing {result.test_name} for better throughput"
                )
            elif result.memory_usage_mb > 1000:
                recommendations.append(f"Optimize memory usage in {result.test_name}")

        # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ¨å¥¨äº‹é …
        avg_cpu = np.mean([r.cpu_usage_percent for r in self.results if r.success])
        if avg_cpu > 80:
            recommendations.append("Consider reducing CPU load or increasing CPU resources")

        avg_memory = np.mean([r.memory_usage_mb for r in self.results if r.success])
        if avg_memory > 2000:
            recommendations.append("Consider optimizing memory usage or increasing RAM")

        return recommendations

    def _save_benchmark_report(self, summary: dict[str, Any]) -> str:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        report_data = {
            "benchmark_info": {
                "timestamp": datetime.now().isoformat(),
                "system_info": self.system_info,
            },
            "summary": summary,
            "detailed_results": [result.__dict__ for result in self.results],
        }

        report_path = os.path.join(self.temp_dir, "benchmark_report.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        return report_path


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("éº»é›€ç‰Œè­œä½œæˆã‚·ã‚¹ãƒ†ãƒ  - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    try:
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark = SystemBenchmark()
        result = benchmark.run_all_benchmarks()

        if result["success"]:
            print("\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {result['summary']['total_tests']}")
            print(f"âœ… æˆåŠŸ: {result['summary']['successful_tests']}")
            print(f"âŒ å¤±æ•—: {result['summary']['failed_tests']}")
            print(f"ğŸ“ˆ æˆåŠŸç‡: {result['summary']['success_rate']:.2%}")
            print(f"â±ï¸  ç·æ™‚é–“: {result['summary']['total_time']:.2f}ç§’")
            print(f"ğŸ† ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {result['summary']['performance_score']:.1f}")

            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
            sys_info = result["system_info"]
            print("\nğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
            print(
                f"  - CPU: {sys_info['cpu_count']} cores ({sys_info['cpu_count_logical']} logical)"
            )
            print(f"  - ãƒ¡ãƒ¢ãƒª: {sys_info['memory_total_gb']:.1f}GB")
            print(f"  - ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {sys_info['platform']}")

            # æ¨å¥¨äº‹é …
            if result["summary"]["recommendations"]:
                print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
                for rec in result["summary"]["recommendations"]:
                    print(f"  - {rec}")

            print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {result['report_path']}")
        else:
            print(f"\nâŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'Unknown error')}")
            return 1

        return 0

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 130
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
